import os
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.anthropic import AnthropicBedrockChatCompletionClient
from bedrock_agentcore.runtime import BedrockAgentCoreApp
from autogen_core.tools import FunctionTool
from autogen_core.models import ModelInfo, ModelFamily

# Uses global inference profile for Claude Sonnet 4.5
# https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles-support.html
MODEL_ID = "global.anthropic.claude-sonnet-4-5-20250929-v1:0"

# Initialize the model client
model_client = AnthropicBedrockChatCompletionClient(
    model=MODEL_ID,
    model_info=ModelInfo(
        vision=False,
        function_calling=True,
        json_output=False,
        family=ModelFamily.CLAUDE_4_SONNET,
        structured_output=True
    ),
    bedrock_info = {"aws_region": os.environ.get("AWS_REGION", "us-east-1")}
)

# Define a simple function tool
def add_numbers(a: int, b: int) -> int:
    """Return the sum of two numbers"""
    return a+b
add_numbers_function_tool = FunctionTool(add_numbers, description="Return the sum of two numbers")

# Integrate with Bedrock AgentCore
app = BedrockAgentCoreApp()

@app.entrypoint
async def main(payload):
    # assume payload input is structured as { "prompt": "<user input>" }

    # Define an AssistantAgent with the model and tool
    agent = AssistantAgent(
        name="{{ agent_name }}",
        model_client=model_client,
        tools=[add_numbers_function_tool] + tools,
        system_message="You are a helpful assistant."
    )

    # Process the user prompt
    prompt = payload.get("prompt", "What is Agentic AI?")

    # Run the agent
    result = await agent.run(task=prompt)

    # Return result
    return {"result": result.messages[-1].content}


if __name__ == "__main__":
    app.run()
