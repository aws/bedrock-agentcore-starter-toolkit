from agents import Agent, Runner, function_tool
from bedrock_agentcore.runtime import BedrockAgentCoreApp
import logging
import sys
import os

if os.getenv("LOCAL_DEV") == "1":
    @asynccontextmanager
    async def async_nullcontext(result):
        yield result
    mcp_server = async_nullcontext([])
else:
    from mcp_client.client import get_streamable_http_mcp_client
    # Import AgentCore Gateway as Streamable HTTP MCP Server
    mcp_server = get_streamable_http_mcp_client()

# Set up logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

# Define a simple function tool
@function_tool
def add_numbers(a: int, b: int) -> int:
    """Return the sum of two numbers"""
    return a+b


# Define an Agent with tools
async def main(query):
    try:
        async with mcp_server as server:
            # Currently defaults to GPT-4.1
            # https://openai.github.io/openai-agents-python/models/
            agent = Agent(
                name="{{ agent_name }}",
                mcp_servers=[server],
                tools=[add_numbers]
            )
            result = await Runner.run(agent, query)
            return result
    except Exception as e:
        logger.error(f"Error during agent execution: {e}", exc_info=True)
        raise e

# Integrate with Bedrock AgentCore
app = BedrockAgentCoreApp()

@app.entrypoint
async def agent_invocation(payload, context):
    # assume payload input is structured as { "prompt": "<user input>" }

    # Process the user prompt
    prompt = payload.get("prompt", "What is Agentic AI?")

    # Run the agent
    result = await main(prompt)

    # Return result
    return {"result": result.final_output}


if __name__== "__main__":
    app.run()
