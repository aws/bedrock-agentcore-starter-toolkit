{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Integration Tests - Notebook API\n",
    "\n",
    "This notebook tests evaluation using the notebook interface that mirrors CLI commands.\n",
    "\n",
    "## Setup\n",
    "Configure your agent ID and session ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## üöÄ Quick Start\n\n**New to evaluation?** Here's the minimal code to get started:\n\n```python\nfrom bedrock_agentcore_starter_toolkit import Evaluation\n\n# Initialize\neval_client = Evaluation(agent_id=\"YOUR_AGENT_ID\", region=\"us-east-1\")\n\n# Run evaluation\nresults = eval_client.run(session_id=\"YOUR_SESSION_ID\")\n\n# View results\nfor result in results.get_successful_results():\n    print(f\"{result.evaluator_name}: {result.value:.2f} - {result.label}\")\n```\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Evaluation\n",
    "\n",
    "# Test configuration - UPDATE THESE VALUES\n",
    "TEST_AGENT_ID = \"test_eval_1-Ux9OE986P4\"  # Replace with your agent ID\n",
    "TEST_SESSION_ID = \"cc8a8e69-8bed-4e5f-9a06-9a58550fd713\"  # Replace with your session ID\n",
    "TEST_REGION = \"us-east-1\"  # Update with your AWS region\n",
    "\n",
    "print(\"‚úÖ Configuration:\")\n",
    "print(f\"  Agent ID: {TEST_AGENT_ID}\")\n",
    "print(f\"  Session ID: {TEST_SESSION_ID}\")\n",
    "print(f\"  Region: {TEST_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Initialize Evaluation\n",
    "\n",
    "Create evaluation instance with agent_id and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with explicit agent_id and region\n",
    "eval_client = Evaluation(agent_id=TEST_AGENT_ID, region=TEST_REGION)\n",
    "\n",
    "print(\"‚úÖ Test 1 PASSED: Evaluation initialized\")\n",
    "print(f\"Agent ID: {eval_client.agent_id}\")\n",
    "print(f\"Region: {eval_client.region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: eval_client.list_evaluators() - List All Evaluators\n",
    "\n",
    "List all available evaluators (builtin and custom).\n",
    "\n",
    "Equivalent to: `agentcore eval evaluator list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all evaluators\n",
    "evaluators_response = eval_client.list_evaluators()\n",
    "\n",
    "evaluators = evaluators_response.get('evaluators', [])\n",
    "print(f\"\\n‚úÖ Test 2 PASSED: Found {len(evaluators)} evaluators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: eval_client.get_evaluator() - Get Evaluator Details\n",
    "\n",
    "Get detailed information about a specific evaluator.\n",
    "\n",
    "Equivalent to: `agentcore eval evaluator get Builtin.Helpfulness`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Test 4: eval_client.run() - Run Evaluation with Default Evaluator\n\nRun evaluation on a session with default evaluator (Builtin.GoalSuccessRate).\n\nEquivalent to: `agentcore eval run --session-id <session>`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Run evaluation with default evaluator\nresults = eval_client.run(session_id=TEST_SESSION_ID)\n\nprint(f\"\\n‚úÖ Test 4 PASSED: Evaluation completed\")\nprint(f\"Session ID: {results.session_id}\")\nprint(f\"Results count: {len(results.results)}\")\n\n# Show successful results with details\nsuccessful = results.get_successful_results()\nprint(f\"Successful: {len(successful)}\")\n\nif successful:\n    result = successful[0]\n    print(f\"\\nüìä Evaluation Result:\")\n    print(f\"  Evaluator: {result.evaluator_name}\")\n    print(f\"  Score: {result.value:.2f}\")\n    print(f\"  Label: {result.label}\")\n    if result.explanation:\n        print(f\"  Explanation: {result.explanation[:200]}...\")\n    if result.token_usage:\n        print(f\"  Tokens: {result.token_usage.get('totalTokens', 0):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Test 5: eval_client.run() - Run with Multiple Evaluators\n\nRun evaluation with multiple evaluators.\n\nEquivalent to: `agentcore eval run -e Builtin.GoalSuccessRate -e Builtin.Accuracy`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Run with multiple evaluators\nresults = eval_client.run(\n    session_id=TEST_SESSION_ID,\n    evaluators=[\"Builtin.GoalSuccessRate\", \"Builtin.Accuracy\"]\n)\n\nprint(f\"\\n‚úÖ Test 5 PASSED: Multi-evaluator run completed\")\nprint(f\"Results count: {len(results.results)}\")\n\n# Show comparison of multiple evaluators\nsuccessful = results.get_successful_results()\nif successful:\n    print(f\"\\nüìä Evaluator Comparison:\")\n    print(f\"{'Evaluator':<30} {'Score':<10} {'Label':<20}\")\n    print(\"-\" * 60)\n    for result in successful:\n        print(f\"{result.evaluator_name:<30} {result.value:<10.2f} {result.label:<20}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with multiple evaluators\n",
    "results = eval_client.run(\n",
    "    session_id=TEST_SESSION_ID,\n",
    "    evaluators=[\"Builtin.Helpfulness\", \"Builtin.Accuracy\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Test 5 PASSED: Multi-evaluator run completed\")\n",
    "print(f\"Results count: {len(results.results)}\")\n",
    "\n",
    "# Show evaluators used\n",
    "evaluator_names = {r.evaluator_name for r in results.results}\n",
    "print(f\"Evaluators used: {evaluator_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: eval_client.run() - Evaluate Specific Trace\n",
    "\n",
    "Evaluate only a specific trace (with previous traces for context).\n",
    "\n",
    "Equivalent to: `agentcore eval run --trace-id <trace>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get a trace ID from observability\n",
    "from bedrock_agentcore_starter_toolkit import Observability\n",
    "\n",
    "obs = Observability(agent_id=TEST_AGENT_ID, region=TEST_REGION)\n",
    "trace_data = obs.list(session_id=TEST_SESSION_ID)\n",
    "trace_ids = list(trace_data.traces.keys())\n",
    "\n",
    "if trace_ids:\n",
    "    TEST_TRACE_ID = trace_ids[0]\n",
    "    print(f\"Using trace ID: {TEST_TRACE_ID}\")\n",
    "    \n",
    "    # Run evaluation on specific trace\n",
    "    results = eval_client.run(\n",
    "        session_id=TEST_SESSION_ID,\n",
    "        trace_id=TEST_TRACE_ID\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Test 6 PASSED: Trace-specific evaluation completed\")\n",
    "    print(f\"Trace ID: {results.trace_id}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Test 6 SKIPPED: No traces found\")\n",
    "    TEST_TRACE_ID = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: eval_client.run() - Export to JSON\n",
    "\n",
    "Run evaluation and export results to JSON file.\n",
    "\n",
    "Equivalent to: `agentcore eval run --output results.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temp file for output\n",
    "output_file = Path(tempfile.gettempdir()) / \"test_eval_results.json\"\n",
    "\n",
    "# Run evaluation with output\n",
    "results = eval_client.run(\n",
    "    session_id=TEST_SESSION_ID,\n",
    "    output=str(output_file)\n",
    ")\n",
    "\n",
    "# Verify file exists\n",
    "assert output_file.exists(), \"Output file not created\"\n",
    "\n",
    "print(f\"\\n‚úÖ Test 7 PASSED: Results exported to {output_file}\")\n",
    "print(f\"File size: {output_file.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Use helper to create custom evaluator config\ncustom_config = create_evaluator_config(\n    instructions=(\n        \"You are an objective judge evaluating the conciseness of an AI assistant's response. \"\n        \"Your task is to assess whether the response is appropriately concise and to the point \"\n        \"without losing essential information. IMPORTANT: Evaluate based on information density \"\n        \"and relevance, not on length alone. A longer response can still be concise if all \"\n        \"information is essential. # Conversation Context: ## Previous turns: {context} \"\n        \"## Target turn to evaluate: {assistant_turn}\"\n    ),\n    rating_scale=[\n        (0.0, \"Very Verbose\", \"Response is overly wordy and rambling. Contains excessive unnecessary details.\"),\n        (0.33, \"Somewhat Verbose\", \"Response has some unnecessary details but generally stays on topic.\"),\n        (0.67, \"Somewhat Concise\", \"Response is reasonably concise with minimal unnecessary information.\"),\n        (1.0, \"Very Concise\", \"Response is perfectly concise and to the point without losing essential information.\"),\n    ]\n)\n\n# Create evaluator\nimport time\ncustom_name = f\"test_conciseness_{int(time.time())}\"\n\nresponse = eval_client.create_evaluator(\n    name=custom_name,\n    config=custom_config,\n    level=\"TRACE\",\n    description=\"Test evaluator for conciseness (created by notebook test)\"\n)\n\nCUSTOM_EVALUATOR_ID = response.get('evaluatorId')\nprint(f\"\\n‚úÖ Test 8 PASSED: Custom evaluator created\")\nprint(f\"Evaluator ID: {CUSTOM_EVALUATOR_ID}\")\nprint(f\"\\nüí° Tip: Use create_evaluator_config() helper to simplify config creation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8: Create Custom Evaluator\n",
    "\n",
    "Create a custom evaluator with configuration.\n",
    "\n",
    "Equivalent to: `agentcore eval evaluator create`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define custom evaluator config using realistic template structure\ncustom_config = {\n    \"llmAsAJudge\": {\n        \"modelConfig\": {\n            \"bedrockEvaluatorModelConfig\": {\n                \"modelId\": \"global.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n                \"inferenceConfig\": {\n                    \"maxTokens\": 500,\n                    \"temperature\": 1.0\n                }\n            }\n        },\n        \"ratingScale\": {\n            \"numerical\": [\n                {\n                    \"value\": 0.0,\n                    \"definition\": \"Response is overly wordy and rambling. Contains excessive unnecessary details.\",\n                    \"label\": \"Very Verbose\"\n                },\n                {\n                    \"value\": 0.33,\n                    \"definition\": \"Response has some unnecessary details but generally stays on topic.\",\n                    \"label\": \"Somewhat Verbose\"\n                },\n                {\n                    \"value\": 0.67,\n                    \"definition\": \"Response is reasonably concise with minimal unnecessary information.\",\n                    \"label\": \"Somewhat Concise\"\n                },\n                {\n                    \"value\": 1.0,\n                    \"definition\": \"Response is perfectly concise and to the point without losing essential information.\",\n                    \"label\": \"Very Concise\"\n                }\n            ]\n        },\n        \"instructions\": \"You are an objective judge evaluating the conciseness of an AI assistant's response. Your task is to assess whether the response is appropriately concise and to the point without losing essential information. IMPORTANT: Evaluate based on information density and relevance, not on length alone. A longer response can still be concise if all information is essential. # Conversation Context: ## Previous turns: {context} ## Target turn to evaluate: {assistant_turn}\"\n    }\n}\n\n# Create evaluator\nimport time\ncustom_name = f\"test_conciseness_{int(time.time())}\"\n\nresponse = eval_client.create_evaluator(\n    name=custom_name,\n    config=custom_config,\n    level=\"TRACE\",\n    description=\"Test evaluator for conciseness (created by notebook test)\"\n)\n\nCUSTOM_EVALUATOR_ID = response.get('evaluatorId')\nprint(f\"\\n‚úÖ Test 8 PASSED: Custom evaluator created\")\nprint(f\"Evaluator ID: {CUSTOM_EVALUATOR_ID}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 9: Run Evaluation with Custom Evaluator\n",
    "\n",
    "Use the newly created custom evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if 'CUSTOM_EVALUATOR_ID' in locals() and CUSTOM_EVALUATOR_ID:\n    # Run evaluation with custom evaluator\n    results = eval_client.run(\n        session_id=TEST_SESSION_ID,\n        evaluators=[CUSTOM_EVALUATOR_ID]\n    )\n    \n    print(f\"\\n‚úÖ Test 9 PASSED: Custom evaluator executed\")\n    print(f\"Results count: {len(results.results)}\")\n    \n    # Show results from custom evaluator\n    successful = results.get_successful_results()\n    failed = results.get_failed_results()\n    \n    print(f\"Successful evaluations: {len(successful)}\")\n    print(f\"Failed evaluations: {len(failed)}\")\n    \n    if successful:\n        result = successful[0]\n        print(f\"\\nüìä Custom Evaluator Result:\")\n        print(f\"  Evaluator: {result.evaluator_name}\")\n        print(f\"  Score: {result.value}\")\n        print(f\"  Label: {result.label}\")\n        if result.explanation:\n            print(f\"  Explanation: {result.explanation[:150]}...\")\n    \n    if failed:\n        print(f\"\\n‚ö†Ô∏è  Some evaluations failed:\")\n        for result in failed:\n            print(f\"  - {result.evaluator_name}: {result.error}\")\nelse:\n    print(\"‚ö†Ô∏è  Test 9 SKIPPED: Custom evaluator not available\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 10: Update Custom Evaluator\n",
    "\n",
    "Update the custom evaluator description.\n",
    "\n",
    "Equivalent to: `agentcore eval evaluator update`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'CUSTOM_EVALUATOR_ID' in locals() and CUSTOM_EVALUATOR_ID:\n",
    "    # Update description\n",
    "    response = eval_client.update_evaluator(\n",
    "        evaluator_id=CUSTOM_EVALUATOR_ID,\n",
    "        description=\"Updated: Test evaluator for conciseness (modified by notebook test)\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Test 10 PASSED: Evaluator updated\")\n",
    "    print(f\"Updated at: {response.get('updatedAt')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Test 10 SKIPPED: Custom evaluator not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 11: Get Custom Evaluator Details\n",
    "\n",
    "Retrieve details of the custom evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'CUSTOM_EVALUATOR_ID' in locals() and CUSTOM_EVALUATOR_ID:\n",
    "    # Get evaluator details\n",
    "    details = eval_client.get_evaluator(CUSTOM_EVALUATOR_ID)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Test 11 PASSED: Retrieved custom evaluator details\")\n",
    "    print(f\"Name: {details.get('evaluatorName')}\")\n",
    "    print(f\"Description: {details.get('description')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Test 11 SKIPPED: Custom evaluator not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 12: Delete Custom Evaluator\n",
    "\n",
    "Clean up by deleting the test evaluator.\n",
    "\n",
    "Equivalent to: `agentcore eval evaluator delete`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'CUSTOM_EVALUATOR_ID' in locals() and CUSTOM_EVALUATOR_ID:\n",
    "    # Delete evaluator\n",
    "    eval_client.delete_evaluator(CUSTOM_EVALUATOR_ID)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Test 12 PASSED: Evaluator deleted\")\n",
    "    print(f\"Deleted evaluator ID: {CUSTOM_EVALUATOR_ID}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Test 12 SKIPPED: Custom evaluator not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 13: Initialize from Config\n",
    "\n",
    "Test initializing evaluation client from config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try initializing from config\n",
    "try:\n",
    "    eval_from_config = Evaluation.from_config()\n",
    "    print(f\"\\n‚úÖ Test 13 PASSED: Initialized from config\")\n",
    "    print(f\"Agent ID: {eval_from_config.agent_id}\")\n",
    "    print(f\"Session ID: {eval_from_config.session_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Test 13 SKIPPED: No config found ({e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 14: Export Evaluator Config to JSON\n",
    "\n",
    "Export evaluator configuration to JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export builtin evaluator config\n",
    "export_file = Path(tempfile.gettempdir()) / \"test_evaluator_config.json\"\n",
    "\n",
    "details = eval_client.get_evaluator(\n",
    "    \"Builtin.Helpfulness\",\n",
    "    output=str(export_file)\n",
    ")\n",
    "\n",
    "assert export_file.exists(), \"Export file not created\"\n",
    "\n",
    "print(f\"\\n‚úÖ Test 14 PASSED: Evaluator config exported\")\n",
    "print(f\"File: {export_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Display test results summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ EVALUATION INTEGRATION TEST SUITE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nTested Notebook API Commands:\")\n",
    "print(\"  ‚úÖ eval_client.run(session_id)                    ‚Üí Run evaluation\")\n",
    "print(\"  ‚úÖ eval_client.run(evaluators=[...])              ‚Üí Multiple evaluators\")\n",
    "print(\"  ‚úÖ eval_client.run(trace_id=...)                  ‚Üí Trace-specific eval\")\n",
    "print(\"  ‚úÖ eval_client.run(output='file.json')            ‚Üí Export results\")\n",
    "print(\"  ‚úÖ eval_client.list_evaluators()                  ‚Üí List evaluators\")\n",
    "print(\"  ‚úÖ eval_client.get_evaluator(id)                  ‚Üí Get details\")\n",
    "print(\"  ‚úÖ eval_client.create_evaluator(name, config)     ‚Üí Create custom\")\n",
    "print(\"  ‚úÖ eval_client.update_evaluator(id, ...)          ‚Üí Update custom\")\n",
    "print(\"  ‚úÖ eval_client.delete_evaluator(id)               ‚Üí Delete custom\")\n",
    "print(\"  ‚úÖ Evaluation.from_config()                       ‚Üí Init from config\")\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Agent ID: {TEST_AGENT_ID}\")\n",
    "print(f\"  Session ID: {TEST_SESSION_ID}\")\n",
    "print(f\"  Region: {TEST_REGION}\")\n",
    "\n",
    "print(\"\\nüí° API matches CLI commands:\")\n",
    "print(\"  CLI: agentcore eval run --session-id abc123\")\n",
    "print(\"  API: eval_client.run(session_id='abc123')\")\n",
    "print(\"\")\n",
    "print(\"  CLI: agentcore eval evaluator list\")\n",
    "print(\"  API: eval_client.list_evaluators()\")\n",
    "print(\"\")\n",
    "print(\"  CLI: agentcore eval evaluator create my-eval --config config.json\")\n",
    "print(\"  API: eval_client.create_evaluator('my-eval', config)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
